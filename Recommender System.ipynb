{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the required libraries\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim \n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the dataset\n",
    "test_set=pd.read_csv('ml-1m/test_set.csv')\n",
    "training_set=pd.read_csv('ml-1m/training_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 250088 entries, 0 to 250087\n",
      "Data columns (total 4 columns):\n",
      "User         250088 non-null int64\n",
      "Movie        250088 non-null int64\n",
      "Rating       250088 non-null int64\n",
      "Timestamp    250088 non-null int64\n",
      "dtypes: int64(4)\n",
      "memory usage: 7.6 MB\n"
     ]
    }
   ],
   "source": [
    "#checking the info of the datasets\n",
    "test_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 750121 entries, 0 to 750120\n",
      "Data columns (total 4 columns):\n",
      "User         750121 non-null int64\n",
      "Movie        750121 non-null int64\n",
      "Rating       750121 non-null int64\n",
      "Timestamp    750121 non-null int64\n",
      "dtypes: int64(4)\n",
      "memory usage: 22.9 MB\n"
     ]
    }
   ],
   "source": [
    "training_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the dataframe to numpy array\n",
    "training_set=np.array(training_set,dtype=int)\n",
    "test_set=np.array(test_set,dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the max value number of movie and number of user\n",
    "nf_users=max(max(training_set[:,0]),max(test_set[:,0]))\n",
    "nf_movies=max(max(training_set[:,1]),max(test_set[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertingthe array to the list of list where one rows corresponds to \n",
    "#the reviews of all the movies that were given by one single users\n",
    "def Converter(data):\n",
    "    dataset=[]\n",
    "    for user in range(1,nf_users+1):\n",
    "        movies=data[:,1][data[:,0]==user]\n",
    "        ratings=data[:,2][data[:,0]==user]\n",
    "        new_ratings=np.zeros(nf_movies)\n",
    "        new_ratings[movies-1]=ratings\n",
    "        dataset.append(list(new_ratings))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set=Converter(training_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set=Converter(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now Convertng these list of list structure to the pytorch tensors\n",
    "training_set=torch.FloatTensor(training_set)\n",
    "test_set=torch.FloatTensor(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now building the Architecture of the Auto Encoder Model\n",
    "#We will use inheritance to build the architecture from the nn Module class\n",
    "#we are bulding six layer 3 layer for encoding and 3 layers for the decoding\n",
    "#we can add more layers depending upon the requirements.\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(AutoEncoder,self).__init__()\n",
    "        self.full_con1=nn.Linear(nf_movies,40)#encoding\n",
    "        self.full_con2=nn.Linear(40,20)#encoding\n",
    "        self.full_con3=nn.Linear(20,10)#encoding\n",
    "        self.full_con4=nn.Linear(10,20)#decoding\n",
    "        self.full_con5=nn.Linear(20,40)#decoding\n",
    "        self.full_con6=nn.Linear(40,nf_movies)#output layer\n",
    "        self.activation=nn.Sigmoid()\n",
    "    def forward(self,x):\n",
    "        x=self.activation(self.full_con1(x))\n",
    "        x=self.activation(self.full_con2(x))\n",
    "        x=self.activation(self.full_con3(x))\n",
    "        x=self.activation(self.full_con4(x))\n",
    "        x=self.activation(self.full_con5(x))\n",
    "        x=self.full_con6(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the object of the class\n",
    "#making the creterion to calculate the loss\n",
    "#making the optimization for changing the values of thne weights\n",
    "auto_encoder=AutoEncoder()\n",
    "criterion=nn.MSELoss()\n",
    "optimizer=optim.RMSprop(auto_encoder.parameters(),lr=0.01,weight_decay=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 1  loss - 1.0069671844389132\n",
      "epoch - 2  loss - 0.9996829715624717\n",
      "epoch - 3  loss - 0.9970854003560181\n",
      "epoch - 4  loss - 0.9959575547251359\n",
      "epoch - 5  loss - 0.9949112781384611\n",
      "epoch - 6  loss - 0.99442320017025\n",
      "epoch - 7  loss - 0.9933930486573685\n",
      "epoch - 8  loss - 0.9927753002560413\n",
      "epoch - 9  loss - 0.9924917333713893\n",
      "epoch - 10  loss - 0.9919021124346419\n",
      "epoch - 11  loss - 0.9909534261818992\n",
      "epoch - 12  loss - 0.9900191745968521\n",
      "epoch - 13  loss - 0.9892806040447778\n",
      "epoch - 14  loss - 0.9887828504900665\n",
      "epoch - 15  loss - 0.9880919544732267\n",
      "epoch - 16  loss - 0.9874697779327835\n",
      "epoch - 17  loss - 0.9867508568870808\n",
      "epoch - 18  loss - 0.9859018356466428\n",
      "epoch - 19  loss - 0.9852832498148297\n",
      "epoch - 20  loss - 0.9845996631805589\n",
      "epoch - 21  loss - 0.9837418740376969\n",
      "epoch - 22  loss - 0.9835745564206186\n",
      "epoch - 23  loss - 0.9828617708610328\n",
      "epoch - 24  loss - 0.9824956669000612\n",
      "epoch - 25  loss - 0.9816252873058987\n",
      "epoch - 26  loss - 0.9808865032793712\n",
      "epoch - 27  loss - 0.9807095250231639\n",
      "epoch - 28  loss - 0.9798901876058003\n",
      "epoch - 29  loss - 0.9800543107217818\n",
      "epoch - 30  loss - 0.9793534793375586\n",
      "epoch - 31  loss - 0.9787212247622717\n",
      "epoch - 32  loss - 0.9774754096175692\n",
      "epoch - 33  loss - 0.976702037091904\n",
      "epoch - 34  loss - 0.9801492827476781\n",
      "epoch - 35  loss - 0.9792896142052958\n",
      "epoch - 36  loss - 0.9784797141488418\n",
      "epoch - 37  loss - 0.9773275548088296\n",
      "epoch - 38  loss - 0.9768205923615604\n",
      "epoch - 39  loss - 0.9755161768182187\n",
      "epoch - 40  loss - 0.9731261585523174\n",
      "epoch - 41  loss - 0.9720854365473551\n",
      "epoch - 42  loss - 0.9693105878213575\n",
      "epoch - 43  loss - 0.967463478904903\n",
      "epoch - 44  loss - 0.9680101773639046\n",
      "epoch - 45  loss - 0.9653344365502431\n",
      "epoch - 46  loss - 0.9619549389859737\n",
      "epoch - 47  loss - 0.9673318200916861\n",
      "epoch - 48  loss - 0.965958694812979\n",
      "epoch - 49  loss - 0.9638627993032599\n",
      "epoch - 50  loss - 0.9613459945331393\n",
      "epoch - 51  loss - 0.962662714101286\n",
      "epoch - 52  loss - 0.9581208893570758\n",
      "epoch - 53  loss - 0.9556477911364312\n",
      "epoch - 54  loss - 0.9585855551629641\n",
      "epoch - 55  loss - 0.9563740586744682\n",
      "epoch - 56  loss - 0.9562325490790333\n",
      "epoch - 57  loss - 0.952525945577783\n",
      "epoch - 58  loss - 0.9575359713927645\n",
      "epoch - 59  loss - 0.9547576749686355\n",
      "epoch - 60  loss - 0.9580858489392354\n",
      "epoch - 61  loss - 0.9563099787853202\n",
      "epoch - 62  loss - 0.9525651586349744\n",
      "epoch - 63  loss - 0.9519004005549742\n",
      "epoch - 64  loss - 0.948901815415517\n",
      "epoch - 65  loss - 0.9452233087716022\n",
      "epoch - 66  loss - 0.9401285905158028\n",
      "epoch - 67  loss - 0.9389371937761721\n",
      "epoch - 68  loss - 0.954253135062954\n",
      "epoch - 69  loss - 0.9509879094811778\n",
      "epoch - 70  loss - 0.9463348892420709\n",
      "epoch - 71  loss - 0.9440465811954714\n",
      "epoch - 72  loss - 0.9404198779033033\n",
      "epoch - 73  loss - 0.9392423804374832\n",
      "epoch - 74  loss - 0.9357379260675033\n",
      "epoch - 75  loss - 0.9337936552857679\n",
      "epoch - 76  loss - 0.9327703231559826\n",
      "epoch - 77  loss - 0.931696131537227\n",
      "epoch - 78  loss - 0.9295567543821261\n",
      "epoch - 79  loss - 0.937469545300174\n",
      "epoch - 80  loss - 0.9410549959657029\n",
      "epoch - 81  loss - 0.9376319824362573\n",
      "epoch - 82  loss - 0.9330388863359392\n",
      "epoch - 83  loss - 0.9292398450103957\n",
      "epoch - 84  loss - 0.9275498633096716\n",
      "epoch - 85  loss - 0.926586903022627\n",
      "epoch - 86  loss - 0.9236393226583831\n",
      "epoch - 87  loss - 0.922604138851226\n",
      "epoch - 88  loss - 0.9285310234555083\n",
      "epoch - 89  loss - 0.9338168936296626\n",
      "epoch - 90  loss - 0.9302885592024152\n",
      "epoch - 91  loss - 0.9264166723750711\n",
      "epoch - 92  loss - 0.9230878772605214\n",
      "epoch - 93  loss - 0.9200311332905328\n",
      "epoch - 94  loss - 0.918474972972404\n",
      "epoch - 95  loss - 0.916722269568645\n",
      "epoch - 96  loss - 0.9190060468294718\n",
      "epoch - 97  loss - 0.9233452485131037\n",
      "epoch - 98  loss - 0.9218917028783099\n",
      "epoch - 99  loss - 0.9206029747971529\n",
      "epoch - 100  loss - 0.921588543665549\n",
      "epoch - 101  loss - 0.9191219721210849\n",
      "epoch - 102  loss - 0.9171792584760051\n",
      "epoch - 103  loss - 0.9146542837065276\n",
      "epoch - 104  loss - 0.9141756230142243\n",
      "epoch - 105  loss - 0.9139663223156046\n",
      "epoch - 106  loss - 0.9119688505289703\n",
      "epoch - 107  loss - 0.9103529575407131\n",
      "epoch - 108  loss - 0.9108095954500685\n",
      "epoch - 109  loss - 0.9102194220762873\n",
      "epoch - 110  loss - 0.9092056762006325\n",
      "epoch - 111  loss - 0.9091203670970862\n",
      "epoch - 112  loss - 0.9209797820727768\n",
      "epoch - 113  loss - 0.9198588955701239\n",
      "epoch - 114  loss - 0.91615299275965\n",
      "epoch - 115  loss - 0.9145341722094884\n",
      "epoch - 116  loss - 0.9135154036553214\n",
      "epoch - 117  loss - 0.9097540129517544\n",
      "epoch - 118  loss - 0.9088515237180069\n",
      "epoch - 119  loss - 0.9077510920321303\n",
      "epoch - 120  loss - 0.9069309036026505\n",
      "epoch - 121  loss - 0.9047105627301762\n",
      "epoch - 122  loss - 0.905413671100074\n",
      "epoch - 123  loss - 0.9053992741923187\n",
      "epoch - 124  loss - 0.9050124545506333\n",
      "epoch - 125  loss - 0.903622552027614\n",
      "epoch - 126  loss - 0.9074841538766758\n",
      "epoch - 127  loss - 0.9122181133197005\n",
      "epoch - 128  loss - 0.9095962168094374\n",
      "epoch - 129  loss - 0.9073407686616671\n",
      "epoch - 130  loss - 0.9073069017068585\n",
      "epoch - 131  loss - 0.9060564933456436\n",
      "epoch - 132  loss - 0.9047031731067997\n",
      "epoch - 133  loss - 0.9031427521197816\n",
      "epoch - 134  loss - 0.9071567579811413\n",
      "epoch - 135  loss - 0.912538821041636\n",
      "epoch - 136  loss - 0.9129068913565888\n",
      "epoch - 137  loss - 0.9122177409920312\n",
      "epoch - 138  loss - 0.9107765075645834\n",
      "epoch - 139  loss - 0.9092580529793447\n",
      "epoch - 140  loss - 0.907439024032123\n",
      "epoch - 141  loss - 0.9049118026511902\n",
      "epoch - 142  loss - 0.907789917377289\n",
      "epoch - 143  loss - 0.9080652821013059\n",
      "epoch - 144  loss - 0.9072561725820893\n",
      "epoch - 145  loss - 0.9058299002255294\n",
      "epoch - 146  loss - 0.9042349026349283\n",
      "epoch - 147  loss - 0.9028431347955611\n",
      "epoch - 148  loss - 0.9032466677993325\n",
      "epoch - 149  loss - 0.9014160208677665\n",
      "epoch - 150  loss - 0.9008632076380994\n",
      "epoch - 151  loss - 0.9009136928860382\n",
      "epoch - 152  loss - 0.9004213545077777\n",
      "epoch - 153  loss - 0.9001322482911144\n",
      "epoch - 154  loss - 0.9015165372293487\n",
      "epoch - 155  loss - 0.9078672154362286\n",
      "epoch - 156  loss - 0.9060855810060632\n",
      "epoch - 157  loss - 0.904187119662656\n",
      "epoch - 158  loss - 0.9030305052732528\n",
      "epoch - 159  loss - 0.9015812556631808\n",
      "epoch - 160  loss - 0.9008318429340549\n",
      "epoch - 161  loss - 0.9003299598378388\n",
      "epoch - 162  loss - 0.899457508442245\n",
      "epoch - 163  loss - 0.8990846750666815\n",
      "epoch - 164  loss - 0.8984975757591098\n",
      "epoch - 165  loss - 0.8984689337132273\n",
      "epoch - 166  loss - 0.8982062313732516\n",
      "epoch - 167  loss - 0.8977597319400952\n",
      "epoch - 168  loss - 0.8974807063447202\n",
      "epoch - 169  loss - 0.8970855073388406\n",
      "epoch - 170  loss - 0.8970143727299801\n",
      "epoch - 171  loss - 0.8972771082429656\n",
      "epoch - 172  loss - 0.8972719355508256\n",
      "epoch - 173  loss - 0.8969864203133331\n",
      "epoch - 174  loss - 0.8965966715001855\n",
      "epoch - 175  loss - 0.8963661659299879\n",
      "epoch - 176  loss - 0.8962532884571769\n",
      "epoch - 177  loss - 0.8966637350613369\n",
      "epoch - 178  loss - 0.8957583043504143\n",
      "epoch - 179  loss - 0.8958241131485801\n",
      "epoch - 180  loss - 0.89556756520062\n",
      "epoch - 181  loss - 0.8953701069377851\n",
      "epoch - 182  loss - 0.895261596923169\n",
      "epoch - 183  loss - 0.895272394968046\n",
      "epoch - 184  loss - 0.8952084925720087\n",
      "epoch - 185  loss - 0.8949336109528371\n",
      "epoch - 186  loss - 0.8950252022294212\n",
      "epoch - 187  loss - 0.8953128831016582\n",
      "epoch - 188  loss - 0.8954426680159594\n",
      "epoch - 189  loss - 0.8949904475328175\n",
      "epoch - 190  loss - 0.8946076542489027\n",
      "epoch - 191  loss - 0.8945365426649444\n",
      "epoch - 192  loss - 0.8953367170667093\n",
      "epoch - 193  loss - 0.9023991692698505\n",
      "epoch - 194  loss - 0.9054424954572352\n",
      "epoch - 195  loss - 0.9027650924929762\n",
      "epoch - 196  loss - 0.9012333253164602\n",
      "epoch - 197  loss - 0.9002222735822667\n",
      "epoch - 198  loss - 0.8990578350327492\n",
      "epoch - 199  loss - 0.8984016126311021\n",
      "epoch - 200  loss - 0.897627954426909\n"
     ]
    }
   ],
   "source": [
    "#Training the Auto encoder\n",
    "n_epochs=200\n",
    "for epochs in range(1,n_epochs+1):\n",
    "    train_loss=0\n",
    "    s=0.\n",
    "    for user in range(nf_users):\n",
    "        input=Variable(training_set[user]).unsqueeze(0)\n",
    "        target=input.clone()\n",
    "        if torch.sum(target.data>0)>0:\n",
    "            output=auto_encoder(input)\n",
    "            target.require_grad=False\n",
    "            output[target==0]=0\n",
    "            loss=criterion(output,target)\n",
    "            mean_corrector=nf_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "            #decide the direction updation of weights \n",
    "            loss.backward()\n",
    "            train_loss+=np.sqrt(loss.data.item()*mean_corrector)\n",
    "            s += 1.\n",
    "            #decide the intensity by which the weights will be updated\n",
    "            optimizer.step()\n",
    "    print(f'epoch - {epochs}  loss - {str(train_loss/s)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss - 0.9157940134087428\n"
     ]
    }
   ],
   "source": [
    "#testing the model\n",
    "test_loss = 0\n",
    "s = 0.\n",
    "for user in range(nf_users):\n",
    "    input = Variable(training_set[user]).unsqueeze(0)\n",
    "    target = Variable(test_set[user])\n",
    "    if torch.sum(target.data > 0) > 0:\n",
    "        output = auto_encoder(input)\n",
    "        target.require_grad = False\n",
    "        output[target.reshape(1,-1) == 0] = 0\n",
    "        loss = criterion(output, target)\n",
    "        mean_corrector = nf_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "        test_loss += np.sqrt(loss.data.item()*mean_corrector)\n",
    "        s += 1.\n",
    "print(f'Test Loss - {(test_loss/s)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
